{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNlxD5p8Nh4PquG9usGbpMz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Codedestructor56/Multimodal-Token-Fusion/blob/main/Training_in_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsHS0blKvukX",
        "outputId": "4c8cfa5a-5989-4030-f559-de60db697b79"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat May 25 08:43:03 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8              11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub\n",
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "so_1rqtIxrwp",
        "outputId": "e276fbfc-6db5-4e33-f3b5-fdc6f5e86beb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.2.2)\n",
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "i0BrKY4ux6ts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdUp2jd2vddy",
        "outputId": "fae434e8-a980-4b3a-a082-4e1e5218f113"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader, DistributedSampler\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from dataclasses import dataclass\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional,List\n",
        "from transformers import BertTokenizer, AutoTokenizer, AutoProcessor\n",
        "import shutil\n",
        "import cv2\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "import torch.distributed as dist\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "import torch.multiprocessing as mp\n",
        "from transformers import AutoTokenizer\n",
        "from torch.optim import AdamW\n",
        "import re"
      ],
      "metadata": {
        "id": "jkdS3AhJwQCQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MedicalDatasetCreator():\n",
        "\n",
        "    def __init__(self, dataset_name: str):\n",
        "        self.dataset_name = dataset_name\n",
        "\n",
        "        self.loaded_df = None\n",
        "        self.image = None\n",
        "        self.case_text = None\n",
        "        self.gender = None\n",
        "        self.age = None\n",
        "        self.image_labels = None\n",
        "        self.dataset_length = None\n",
        "\n",
        "    def load_dataset(self):\n",
        "        self.loaded_df = pd.read_csv(os.path.join(self.dataset_name, \"info_dataframe.csv\"))\n",
        "        self.loaded_df.dropna(inplace = True)\n",
        "        self.dataset_length = self.loaded_df.shape[0]\n",
        "\n",
        "    def load_row(self, index: int = 0):\n",
        "        self.image = cv2.imread(os.path.join(self.dataset_name,self.loaded_df[\"file\"].iloc[index]))\n",
        "        self.case_text = self.loaded_df[\"case_text\"].iloc[index]\n",
        "        self.gender = self.loaded_df[\"gender\"].iloc[index]\n",
        "        self.age = self.loaded_df[\"age\"].iloc[index]"
      ],
      "metadata": {
        "id": "Z2CsLlzAv8_l"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZT2Wab270N8N"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Parameters:\n",
        "    device: str\n",
        "    num_heads: int\n",
        "    emb_dim: int\n",
        "    max_seq_len: int\n",
        "    tokenizer: str\n",
        "    max_im_height: int\n",
        "    max_im_width: int\n",
        "    batch_size: int\n",
        "    dataset_name: str\n",
        "    use_cache: str\n",
        "    ffn_hidden_dim: int\n",
        "    thresh: Optional[int]\n",
        "    num_layers: int\n",
        "    vocab_size: int\n",
        "    patch_size: int\n",
        "    token_thresh: int\n",
        "    imp_layer_hidden: int\n",
        "    div_batch: int\n",
        "\n",
        "class Medical_Data(Dataset):\n",
        "    def __init__(self, params:Parameters):\n",
        "        super().__init__()\n",
        "        self.device = params.device\n",
        "        self.max_seq_len = params.max_seq_len\n",
        "        self.emb_dim = params.emb_dim\n",
        "        self.max_height = params.max_im_height\n",
        "        self.max_width = params.max_im_width\n",
        "        self.batch_size = params.batch_size\n",
        "        self.tokenizer = params.tokenizer\n",
        "        self.dataset = MedicalDatasetCreator(params.dataset_name)\n",
        "        self.dataset.load_dataset()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset.dataset_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        self.dataset.load_row(idx)\n",
        "        case_text = self.dataset.case_text\n",
        "        encoded_case_text = torch.tensor(self.tokenizer.encode(case_text),dtype = torch.int32).to(self.device)\n",
        "        image = torch.tensor(self.dataset.image, dtype = torch.int8)\n",
        "\n",
        "        return torch.cat((encoded_case_text[:self.max_seq_len-1], encoded_case_text[-1].unsqueeze(0)), dim=0), image\n",
        "\n",
        "    def pad_im(self, imgs):\n",
        "        img_batch = []\n",
        "        for img in imgs:\n",
        "            img_gray = cv2.cvtColor(np.array(img, dtype=np.uint8), cv2.COLOR_BGR2GRAY)\n",
        "            height, width = img_gray.shape[:2]\n",
        "\n",
        "            pad_height = max(0, self.max_height - height)\n",
        "            pad_width = max(0, self.max_width - width)\n",
        "            top_pad = pad_height // 2\n",
        "            bottom_pad = pad_height - top_pad\n",
        "            left_pad = pad_width // 2\n",
        "            right_pad = pad_width - left_pad\n",
        "            img_padded = cv2.copyMakeBorder(img_gray, top_pad, bottom_pad, left_pad, right_pad, cv2.BORDER_CONSTANT, value=0)\n",
        "            img_cropped = img_padded[:self.max_height, :self.max_width]\n",
        "            img_batch.append(img_cropped)\n",
        "\n",
        "        imgs_tensor = torch.tensor(np.array(img_batch), dtype=torch.uint8).to(self.device)\n",
        "        del img_batch\n",
        "        return imgs_tensor\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        batch = list(zip(*batch))\n",
        "        padded_text = pad_sequence(batch[0], batch_first = True, padding_value = 0)\n",
        "        padded_img = self.pad_im(batch[1])\n",
        "\n",
        "        return padded_text.to(self.device), padded_img\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RotaryEmbeddings(nn.Module):\n",
        "    def __init__(self, device:str, theta: int =10000):\n",
        "        super().__init__()\n",
        "        self.theta = theta\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, x: torch.Tensor, seq_len:Optional[int]=None, emb_dim:Optional[int]=None)->torch.Tensor:\n",
        "        batch_size, seq_len, emb_dim = x.shape\n",
        "        assert emb_dim%2==0, \"Embeddings dimension must be even\"\n",
        "        #Q_i=10000^(-2(i-1)/emb_dim)\n",
        "        thetas = (1.0/self.theta**((2*torch.arange(0,emb_dim,2))//emb_dim)).to(self.device)\n",
        "        thetas_repeated = thetas.unsqueeze(0).repeat(seq_len, 1)\n",
        "        thetas_true = thetas_repeated * (torch.arange(seq_len, device = self.device)+1).unsqueeze(1)\n",
        "        #calculate the rotation matrices using these thetas, apply them on the embeddings in  2D or complex space\n",
        "        matrix_rot = torch.stack((torch.sin(thetas_true),torch.cos(thetas_true)),dim=-1).to(self.device)\n",
        "        comp_matrix = torch.view_as_complex(matrix_rot).unsqueeze(0)\n",
        "        x_reshaped = torch.view_as_complex(x.reshape(batch_size, seq_len, emb_dim//2, 2))\n",
        "        rotated_x = torch.view_as_real(x_reshaped * comp_matrix).squeeze(-1).reshape(batch_size, seq_len, emb_dim).to(self.device)\n",
        "        del x_reshaped, comp_matrix, matrix_rot, thetas_true, thetas_repeated, thetas\n",
        "        torch.cuda.empty_cache()\n",
        "        return rotated_x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, params: Parameters):\n",
        "        super().__init__()\n",
        "        self.use_cache = params.use_cache\n",
        "        self.device = params.device\n",
        "        self.pos_rotor = RotaryEmbeddings(self.device)\n",
        "\n",
        "        self.num_heads = params.num_heads\n",
        "        assert params.emb_dim % self.num_heads==0, \"Make the embedding dim divisible by num_heads\"\n",
        "        self.head_dim = params.emb_dim//self.num_heads\n",
        "        self.wq = nn.Linear(params.emb_dim, self.num_heads*self.head_dim).to(self.device)\n",
        "        self.wk = nn.Linear(params.emb_dim, self.num_heads*self.head_dim).to(self.device)\n",
        "        self.wv = nn.Linear(params.emb_dim, self.num_heads*self.head_dim).to(self.device)\n",
        "        self.wo = nn.Linear(params.emb_dim, self.num_heads*self.head_dim).to(self.device)\n",
        "        if self.use_cache:\n",
        "            self.c_v = torch.zeros((params.max_batch_size, params.max_seq_len, self.num_heads, self.head_dim))\n",
        "            self.c_k = torch.zeros((params.max_batch_size, params.max_seq_len, self.num_heads, self.head_dim))\n",
        "\n",
        "    def forward(self, x:torch.Tensor, cur_pos: Optional[int]=None)->torch.Tensor:\n",
        "        batch_size, seq_len, emb_dim = x.shape\n",
        "        query = self.wq(x)\n",
        "        key = self.wk(x)\n",
        "        value = self.wv(x)\n",
        "        output = self.wo(x)\n",
        "\n",
        "        xq = self.pos_rotor(query).reshape(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        xv = self.pos_rotor(value).reshape(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        xk = key.reshape(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "\n",
        "        if self.use_cache:\n",
        "            self.c_v[:batch_size, cur_pos:cur_pos+seq_len]=xv\n",
        "            self.c_k[:batch_size, cur_pos:cur_pos+seq_len]=xk\n",
        "\n",
        "            keys = self.c_k[:batch_size, :cur_pos+seq_len]\n",
        "            values = self.c_v[:batch_size, :cur_pos+seq_len]\n",
        "\n",
        "            keys = keys[:,:,:,None,:].expand(keys.shape[0], keys.shape[1],\n",
        "                                           self.num_heads, 1, self.head_dim).reshape(keys.shape[0],\n",
        "                                            keys.shape[1], self.num_heads, self.head_dim)\n",
        "\n",
        "            values = values[:,:,:,None,:].expand(values.shape[0], values.shape[1],\n",
        "                                                 self.num_heads, 1, self.head_dim).reshape(values.shape[0],\n",
        "                                                 values.shape[1], self.num_heads, self.head_dim)\n",
        "\n",
        "        else:\n",
        "            keys = xq\n",
        "            values = xv\n",
        "\n",
        "        xq = xq.permute(0, 2, 1, 3).contiguous().to(self.device)\n",
        "        keys = keys.permute(0, 2, 3, 1).contiguous().to(self.device)\n",
        "        values = values.permute(0, 2, 1, 3).contiguous().to(self.device)\n",
        "\n",
        "        query_key_score = torch.matmul(xq, keys)/math.sqrt(self.head_dim)\n",
        "        attention_score = torch.matmul(query_key_score, values).transpose(1,2).contiguous().reshape(batch_size, seq_len, -1)\n",
        "        output = self.wo(attention_score)\n",
        "\n",
        "        del query_key_score, attention_score, xq, keys, values\n",
        "        torch.cuda.empty_cache()\n",
        "        #make sure that the dimensions are correct and that the training and inferencing parts are compatible\n",
        "        return output\n",
        "\n",
        "class RMSnorm(nn.Module):\n",
        "    def __init__(self, dim:int, device:str, thresh: float = 1e-4):\n",
        "        super().__init__()\n",
        "        self.params = nn.Parameter(torch.ones(dim))\n",
        "        self.thresh = thresh\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, x:torch.Tensor)->torch.Tensor:\n",
        "        denom = torch.sqrt(x.pow(2).mean(-1,keepdims=True)).to(self.device)\n",
        "        res = ((x.to(self.device))*self.params.to(self.device))/denom\n",
        "        del denom\n",
        "        torch.cuda.empty_cache()\n",
        "        return res\n",
        "\n",
        "class SwiGLu_Forward(nn.Module):\n",
        "    def __init__(self, params:Parameters):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = params.ffn_hidden_dim\n",
        "        self.device = params.device\n",
        "        self.w1 = nn.Linear(params.emb_dim, self.hidden_dim).to(self.device)\n",
        "        self.w2 = nn.Linear(params.emb_dim, self.hidden_dim).to(self.device)\n",
        "        self.w3 = nn.Linear(self.hidden_dim, params.emb_dim).to(self.device)\n",
        "\n",
        "    def forward(self, x:torch.Tensor)->torch.Tensor:\n",
        "        return self.w3(self.w2(x)*nn.functional.silu(self.w1(x)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class PatchEmbeddings(nn.Module):\n",
        "    def __init__(self, params: Parameters):\n",
        "        super().__init__()\n",
        "        self.device = params.device\n",
        "        self.emb_dim = params.emb_dim\n",
        "        self.patch_size = params.patch_size\n",
        "        self.max_height = params.max_im_height\n",
        "        self.max_width = params.max_im_width\n",
        "        assert self.max_height == self.max_width, \"Width and height should be equal\"\n",
        "        assert self.max_height % self.patch_size == 0, \"Patch size and image dims should be compatible\"\n",
        "        self.linear = nn.Linear(self.patch_size**2, self.emb_dim).to(self.device)\n",
        "\n",
        "    def patchify(self, image: torch.Tensor):\n",
        "        patches = []\n",
        "        batch_size, height, width = image.size()\n",
        "        for b in range(batch_size):\n",
        "            for h in range(0, height, self.patch_size):\n",
        "                for w in range(0, width, self.patch_size):\n",
        "                    patch = image[b,h:h+self.patch_size, w:w+self.patch_size].float()\n",
        "                    patches.append(patch)\n",
        "\n",
        "        return torch.stack(patches, dim = 0).reshape(batch_size, -1, self.patch_size, self.patch_size)\n",
        "\n",
        "    def forward(self, x:torch.Tensor)->torch.tensor:\n",
        "        batch_size, seq_len, patch_height, patch_width = x.size()\n",
        "        #print(patch_height, patch_width)\n",
        "        assert patch_width == patch_height, \"Uniform patch size should be provided\"\n",
        "        patches = self.linear(x.view(batch_size, seq_len, -1, patch_width * patch_height).to(torch.float32).squeeze(2))\n",
        "        positions = torch.arange(patches.shape[1], dtype=torch.float).unsqueeze(1)\n",
        "        pe = torch.zeros(patches.shape[1], self.emb_dim)\n",
        "        div_term = torch.exp(torch.arange(0, self.emb_dim, 2).float() * (-math.log(10000.0) / self.emb_dim))\n",
        "        pe[:, 0::2] = torch.sin(positions * div_term)\n",
        "        pe[:, 1::2] = torch.cos(positions * div_term)\n",
        "\n",
        "        pe = pe.repeat(batch_size, 1, 1, 1).squeeze(1).to(self.device)\n",
        "        patches += pe\n",
        "        del pe, positions, div_term\n",
        "        torch.cuda.empty_cache()\n",
        "        return patches\n",
        "\n"
      ],
      "metadata": {
        "id": "t4JJgxzjwufd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, params: Parameters):\n",
        "        super().__init__()\n",
        "        self.device = params.device\n",
        "        self.emb_dim = params.emb_dim\n",
        "        self.thresh = params.thresh\n",
        "        self.norm = RMSnorm(self.emb_dim, self.device, self.thresh)\n",
        "        self.attention = Attention(params)\n",
        "        self.ffn = SwiGLu_Forward(params)\n",
        "\n",
        "    def forward(self, x:torch.Tensor, cur_pos: Optional[int])->torch.Tensor:\n",
        "        first_layer = x + self.attention(self.norm(x), cur_pos)\n",
        "        second_layer = first_layer + self.ffn(self.norm(first_layer))\n",
        "\n",
        "        del first_layer\n",
        "        torch.cuda.empty_cache()\n",
        "        return second_layer\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, params: Parameters):\n",
        "        super().__init__()\n",
        "        self.layers_enc_text = nn.ModuleList()\n",
        "        self.layers_enc_im = nn.ModuleList()\n",
        "        for _ in range(params.num_layers):\n",
        "            self.layers_enc_text.append(Encoder(params))\n",
        "            self.layers_enc_im.append(Encoder(params))\n",
        "        self.device = params.device\n",
        "        self.emb_dim = params.emb_dim\n",
        "        self.seq_len = params.max_seq_len\n",
        "        self.vocab_size = params.vocab_size\n",
        "        self.text_embeddings = nn.Embedding(self.vocab_size, self.emb_dim).to(self.device)\n",
        "        self.thresh = params.thresh\n",
        "        self.norm = RMSnorm(self.emb_dim, self.device, self.thresh)\n",
        "        self.div_batch = params.div_batch\n",
        "        self.patch_embeddings = PatchEmbeddings(params)\n",
        "        self.linear = nn.Linear(self.emb_dim, self.vocab_size).to(self.device)\n",
        "\n",
        "        self.max_height = params.max_im_height\n",
        "        self.max_width = params.max_im_width\n",
        "        self.patch_size = params.patch_size\n",
        "\n",
        "    def forward(self, x: torch.Tensor, cur_pos: Optional[int], im_inc: bool)->torch.Tensor:\n",
        "        assert self.div_batch<=x.shape[0], \"Batch serializer should not exceed tensor dimensions\"\n",
        "        if im_inc:\n",
        "            im_seq_len = (self.max_height//self.patch_size)*(self.max_width//self.patch_size)\n",
        "            res = self.patch_embeddings(x)\n",
        "            del im_seq_len\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            if cur_pos is None:\n",
        "                res = self.norm(res)\n",
        "                for layer in self.layers_enc_im:\n",
        "                    res = layer(res, cur_pos)\n",
        "\n",
        "                res = torch.chunk(res, self.div_batch, dim = 0)\n",
        "\n",
        "                accumulated_output = None\n",
        "                for chunk_idx in range(len(res)):\n",
        "                    out = self.linear(res[chunk_idx])\n",
        "                    if accumulated_output is None:\n",
        "                        accumulated_output = out\n",
        "                    else:\n",
        "                        # Concatenate the current output with the accumulated output along the specified dimension\n",
        "                        accumulated_output = torch.cat((accumulated_output, out), dim=0)\n",
        "\n",
        "                    del out\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "                del accumulated_output\n",
        "                torch.cuda.empty_cache()\n",
        "            else:\n",
        "                assert res.shape[1]==1, \"Please pass one token at a time\"\n",
        "                res = self.norm(res)\n",
        "                for layer in self.layers_enc_im:\n",
        "                    res = layer(res, cur_pos)\n",
        "                res = self.linear(res)\n",
        "        else:\n",
        "            res = self.text_embeddings(x)\n",
        "            if cur_pos is None:\n",
        "                res = self.norm(res)\n",
        "                for layer in self.layers_enc_text:\n",
        "                    res = layer(res, cur_pos)\n",
        "\n",
        "                res = torch.chunk(res, self.div_batch, dim = 0)\n",
        "\n",
        "                accumulated_output = None\n",
        "                for chunk_idx in range(len(res)):\n",
        "                    out = self.linear(res[chunk_idx])\n",
        "                    if accumulated_output is None:\n",
        "                        accumulated_output = out\n",
        "                    else:\n",
        "                        accumulated_output = torch.cat((accumulated_output, out), dim=0)\n",
        "\n",
        "                    del out\n",
        "                    torch.cuda.empty_cache()\n",
        "            else:\n",
        "                assert res.shape[1]==1, \"Pass one token at a time\"\n",
        "                res = self.norm(res)\n",
        "                for layer in self.layers_enc_text:\n",
        "                    res = layer(res, cur_pos)\n",
        "                res = self.linear(res)\n",
        "\n",
        "        return res\n",
        "\n",
        "class TokenFusion(nn.Module):\n",
        "    def __init__(self, params: Parameters):\n",
        "        super().__init__()\n",
        "        self.device = params.device\n",
        "        self.emb_dim = params.emb_dim\n",
        "        self.token_thresh = params.token_thresh\n",
        "        self.hidden_dim = params.imp_layer_hidden\n",
        "        self.thresh = params.thresh\n",
        "        self.norm = RMSnorm(self.emb_dim, self.device, self.thresh)\n",
        "        self.vocab_size = params.vocab_size\n",
        "        self.seq_len = params.max_seq_len\n",
        "        self.imp_layer1 = nn.Linear(self.seq_len, self.hidden_dim).to(self.device)\n",
        "        self.imp_layer2 = nn.Linear(self.hidden_dim, 1).to(self.device)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.transformer = Transformer(params)\n",
        "\n",
        "\n",
        "    def forward(self, x:Optional[torch.Tensor], y:Optional[torch.Tensor], cur_pos: Optional[int], im_inc: bool):\n",
        "        if cur_pos is None:\n",
        "            x, y = self.transformer(x, cur_pos, False), self.transformer(y, cur_pos, im_inc)\n",
        "            x = torch.stack(x, dim=0).squeeze(1)\n",
        "            y = torch.stack(y, dim=0).squeeze(1)\n",
        "            seq_len1, seq_len2 = x.shape[1], y.shape[1]\n",
        "            min_seq_len = min(seq_len1, seq_len2)\n",
        "            x_fuse, y_fuse, x_rem, y_rem = x[:,:min_seq_len,:], y[:,:min_seq_len,:], x[:,min_seq_len:,:], y[:,min_seq_len:,:]\n",
        "\n",
        "            token_scores_x = self.sigmoid(self.imp_layer2(self.imp_layer1(x_fuse)))\n",
        "            token_scores_y = self.sigmoid(self.imp_layer2(self.imp_layer1(y_fuse)))\n",
        "            mask_x = (token_scores_x > self.token_thresh).int()\n",
        "            inv_mask_x = 1-mask_x\n",
        "            mask_y = (token_scores_y > self.token_thresh).int()\n",
        "            inv_mask_y = 1-mask_y\n",
        "            x_fin = x_fuse * mask_x + y_fuse * inv_mask_x.expand(x_fuse.shape[0],min_seq_len,x_fuse.shape[2])\n",
        "            y_fin = y_fuse * mask_y + x_fuse * inv_mask_y.expand(y_fuse.shape[0],min_seq_len,y_fuse.shape[2])\n",
        "\n",
        "            x_fin = torch.cat((x_fin, x_rem), dim=1).squeeze()\n",
        "            y_fin = torch.cat((y_fin, y_rem), dim=1).squeeze()\n",
        "\n",
        "\n",
        "            del x_fuse, y_fuse, x_rem, y_rem, token_scores_x, token_scores_y, mask_x, mask_y, inv_mask_x, inv_mask_y\n",
        "            torch.cuda.empty_cache()\n",
        "            return x_fin, y_fin\n",
        "        else:\n",
        "            if im_inc:\n",
        "                return self.transformer(y, cur_pos, im_inc)\n",
        "            else:\n",
        "                return self.transformer(x, cur_pos, im_inc)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "x6a_dINKxCaa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method 1:"
      ],
      "metadata": {
        "id": "MbyoeYI21bYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup(rank, world_size):\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    os.environ['MASTER_PORT'] = '12355'\n",
        "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
        "\n",
        "def cleanup():\n",
        "    dist.destroy_process_group()\n",
        "\n",
        "def train(rank, world_size, params):\n",
        "    setup(rank, world_size)\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    dataset = Medical_Data(params)\n",
        "    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank)\n",
        "    dataloader = DataLoader(dataset, batch_size=params.batch_size, sampler=sampler, collate_fn=dataset.collate_fn)\n",
        "    model = TokenFusion(params).to(rank)\n",
        "    model = DDP(model, device_ids=[rank])\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "    model.train()\n",
        "    for epoch in range(3):\n",
        "        sampler.set_epoch(epoch)\n",
        "        for batch in dataloader:\n",
        "            texts, images = batch\n",
        "            pt = PatchEmbeddings(params)\n",
        "            patched_images = pt.patchify(images)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(texts, patched_images, cur_pos=None, im_inc=True)\n",
        "            probs = [F.softmax(outputs[0], dim=-1), F.softmax(outputs[1], dim=-1)]\n",
        "            embeds = nn.Embedding(texts.shape[-1], params.emb_dim).to(params.device)\n",
        "            texts = embeds(texts)\n",
        "\n",
        "            if probs[0].shape[1] < texts.shape[1]:\n",
        "                probs[0] = torch.nn.functional.pad(probs[0], (0, 0, 0, texts.shape[1]-probs[0].shape[1]))\n",
        "\n",
        "            if probs[1].shape[1] < texts.shape[1]:\n",
        "                probs[1] = torch.nn.functional.pad(probs[1], (0, 0, 0, texts.shape[1]-probs[1].shape[1]))\n",
        "\n",
        "            print(texts.shape, probs[0].shape, probs[1].shape)\n",
        "            loss_text1 = F.cross_entropy(probs[0], texts)\n",
        "            loss_text2 = F.cross_entropy(probs[1], texts)\n",
        "            loss = (loss_text1 + loss_text2) / 2.0\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if rank == 0:\n",
        "                print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
        "\n",
        "    cleanup()\n",
        "\n",
        "def get_num_gpus():\n",
        "    if torch.cuda.is_available():\n",
        "        num_gpus = torch.cuda.device_count()\n",
        "    else:\n",
        "        num_gpus = 0\n",
        "    return num_gpus\n",
        "\n",
        "def main():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
        "    print(tokenizer.vocab_size)\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    params = Parameters(device = device, use_cache = False, num_heads = 16, thresh = None, emb_dim = 256, max_seq_len = 256\n",
        "                    ,ffn_hidden_dim = 512, batch_size = 8, div_batch = 8,\n",
        "                    tokenizer = tokenizer, vocab_size = tokenizer.vocab_size+1,\n",
        "                    max_im_width = 240, max_im_height = 240, num_layers = 1, patch_size = 16, dataset_name = \"/content/drive/MyDrive/ct_scan_data\",\n",
        "                    token_thresh = 0.3, imp_layer_hidden = 512)\n",
        "\n",
        "    world_size = get_num_gpus()\n",
        "    print(f\"Num of GPUs: {world_size}\")\n",
        "    mp.spawn(train, args=(world_size, params), nprocs=world_size, join=True)\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "E-_IMj3VxLUb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tensorboard --logdir=runs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNA43Jw01OXH",
        "outputId": "45fbca2b-84c8-4afb-a433-df8e3baa2525"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-05-25 08:44:10.278523: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-25 08:44:10.278594: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-25 08:44:10.280162: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-25 08:44:12.636292: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "NOTE: Using experimental fast data loading logic. To disable, pass\n",
            "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
            "    https://github.com/tensorflow/tensorboard/issues/4784\n",
            "\n",
            "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
            "TensorBoard 2.15.2 at http://localhost:6006/ (Press CTRL+C to quit)\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method 2:"
      ],
      "metadata": {
        "id": "z0hSD6a_1e3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import math\n",
        "\n",
        "def train(params):\n",
        "    torch.manual_seed(42)\n",
        "    dataset = Medical_Data(params)\n",
        "    dataloader = DataLoader(dataset, batch_size=params.batch_size, shuffle=True, collate_fn=dataset.collate_fn)\n",
        "    model = TokenFusion(params).to(params.device)\n",
        "    optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "    model.train()\n",
        "    writer = SummaryWriter()\n",
        "\n",
        "    for epoch in range(3):\n",
        "        epoch_loss = 0\n",
        "        for batch_idx, batch in enumerate(dataloader):\n",
        "            texts, images = batch\n",
        "            pt = PatchEmbeddings(params)\n",
        "            patched_images = pt.patchify(images)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(texts, patched_images, cur_pos=None, im_inc=True)\n",
        "            probs = [F.softmax(outputs[0], dim=-1), F.softmax(outputs[1], dim=-1)]\n",
        "            embeds = nn.Embedding(params.vocab_size, params.emb_dim).to(params.device)\n",
        "            texts = embeds(texts)\n",
        "            if probs[0].shape[1] < texts.shape[1]:\n",
        "                probs[0] = torch.nn.functional.pad(probs[0], (0, 0, 0, texts.shape[1] - probs[0].shape[1]))\n",
        "            if probs[1].shape[1] < texts.shape[1]:\n",
        "                probs[1] = torch.nn.functional.pad(probs[1], (0, 0, 0, texts.shape[1] - probs[1].shape[1]))\n",
        "            loss_text1 = F.cross_entropy(probs[0], texts)\n",
        "            loss_text2 = F.cross_entropy(probs[1], texts)\n",
        "            loss = (loss_text1 + loss_text2) / 2.0\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # Log training loss\n",
        "            writer.add_scalar('Loss/train', loss.item(), epoch * len(dataloader) + batch_idx)\n",
        "\n",
        "            # Calculate and log perplexity\n",
        "            perplexity = math.exp(loss.item())\n",
        "            writer.add_scalar('Perplexity/train', perplexity, epoch * len(dataloader) + batch_idx)\n",
        "\n",
        "            print(f\"Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item()}, Perplexity: {perplexity}\")\n",
        "\n",
        "        avg_epoch_loss = epoch_loss / len(dataloader)\n",
        "        writer.add_scalar('Loss/epoch', avg_epoch_loss, epoch)\n",
        "\n",
        "        avg_epoch_perplexity = math.exp(avg_epoch_loss)\n",
        "        writer.add_scalar('Perplexity/epoch', avg_epoch_perplexity, epoch)\n",
        "\n",
        "        print(f\"Epoch: {epoch}, Average Loss: {avg_epoch_loss}, Average Perplexity: {avg_epoch_perplexity}\")\n",
        "\n",
        "    torch.save(model.state_dict(), 'token_fusion_model.pth')\n",
        "    writer.add_text(\"Model Save\", \"Model saved as token_fusion_model.pth\")\n",
        "    writer.close()\n",
        "    print(\"Model saved as token_fusion_model.pth\")\n",
        "\n",
        "def main():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
        "    print(tokenizer.vocab_size)\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    params = Parameters(\n",
        "        device=device,\n",
        "        use_cache=False,\n",
        "        num_heads=16,\n",
        "        thresh=None,\n",
        "        emb_dim=256,\n",
        "        max_seq_len=256,\n",
        "        ffn_hidden_dim=512,\n",
        "        batch_size=8,\n",
        "        div_batch=8,\n",
        "        tokenizer=tokenizer,\n",
        "        vocab_size=tokenizer.vocab_size + 1,\n",
        "        max_im_width=240,\n",
        "        max_im_height=240,\n",
        "        num_layers=1,\n",
        "        patch_size=16,\n",
        "        dataset_name=\"/content/drive/MyDrive/ct_scan_data\",\n",
        "        token_thresh=0.3,\n",
        "        imp_layer_hidden=512\n",
        "    )\n",
        "    train(params)\n",
        "\n",
        "main()\n"
      ],
      "metadata": {
        "id": "K09DWgQS0-aH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('token_fusion_model.pth')\n"
      ],
      "metadata": {
        "id": "ZxD6dtns1Tve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference:\n"
      ],
      "metadata": {
        "id": "b_Xtyjpk22PT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def _sample_top_p(probs, p):\n",
        "    sorted_probs, sorted_indices = torch.sort(probs, dim=-1, descending=True)\n",
        "    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "    top_p_mask = cumulative_probs <= p\n",
        "    top_p_mask[..., 0] = True\n",
        "    filtered_probs = sorted_probs * top_p_mask\n",
        "    filtered_probs = filtered_probs / filtered_probs.sum(dim=-1, keepdim=True)\n",
        "    next_token = torch.multinomial(filtered_probs, num_samples=1)\n",
        "    next_token = torch.gather(sorted_indices, -1, next_token)\n",
        "    return next_token\n",
        "\n",
        "\n",
        "def pad_im(img, max_height, max_width, device):\n",
        "    img_gray = cv2.cvtColor(np.array(img, dtype=np.uint8), cv2.COLOR_BGR2GRAY)\n",
        "    height, width = img_gray.shape[:2]\n",
        "    pad_height = max(0, max_height - height)\n",
        "    pad_width = max(0, max_width - width)\n",
        "    top_pad = pad_height // 2\n",
        "    bottom_pad = pad_height - top_pad\n",
        "    left_pad = pad_width // 2\n",
        "    right_pad = pad_width - left_pad\n",
        "\n",
        "    img_padded = cv2.copyMakeBorder(img_gray, top_pad, bottom_pad, left_pad, right_pad, cv2.BORDER_CONSTANT, value=0)\n",
        "    img_cropped = img_padded[:max_height, :max_width]\n",
        "\n",
        "    img_tensor = torch.tensor(np.array(img_cropped), dtype=torch.uint8).to(device)\n",
        "    return img_tensor\n",
        "\n",
        "\n",
        "def infer(tokenizer, prompts: list[str], params: Parameters, model_path: Optional[str], model: TokenFusion , temp = 0.3, top_p = 0.8):\n",
        "    max_len = params.max_seq_len\n",
        "    batch_size = params.batch_size\n",
        "    device = params.device\n",
        "    image_paths = []\n",
        "    text_descriptions = []\n",
        "\n",
        "\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    for prompt in prompts:\n",
        "        if \"image: \" in prompt:\n",
        "            image_part, text_part = prompt.split(\" text: \")\n",
        "            image_path = image_part.replace(\"image: \", \"\")\n",
        "        else:\n",
        "            image_path = None\n",
        "            text_part = prompt.replace(\"text: \", \"\")\n",
        "\n",
        "        image_paths.append(image_path)\n",
        "        text_descriptions.append(text_part)\n",
        "\n",
        "    prompts = [tokenizer.encode(prompt) for prompt in text_descriptions]\n",
        "    assert len(prompts)<=batch_size, f\"Too many prompts, they should be less than or equal to{batch_size}\"\n",
        "    max_prompt_len = max(len(prompt) for prompt in prompts)\n",
        "    assert max_prompt_len<=max_len, f\"Keep your prompt size below {max_len}\"\n",
        "\n",
        "    total_len = min(params.max_seq_len, max_len + max_prompt_len)\n",
        "    pad_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
        "    tokens = torch.full((batch_size, total_len), pad_id, dtype=torch.long, device=device)\n",
        "    for k, t in enumerate(prompts):\n",
        "        tokens[k, :len(t)] = torch.tensor(t, dtype=torch.long, device=device)\n",
        "\n",
        "    only_text_indices = []\n",
        "    for i in range(len(image_paths)):\n",
        "        if image_paths[i] is not None:\n",
        "            only_text_indices.append(i)\n",
        "\n",
        "    only_text_indices = torch.tensor(only_text_indices).to(device)\n",
        "\n",
        "    images = []\n",
        "    for path in image_paths:\n",
        "        if path is None:\n",
        "            images.append(torch.zeros(params.max_im_height, params.max_im_width))\n",
        "        else:\n",
        "            images.append(pad_im(cv2.imread(path), params.max_im_height, params.max_im_width, device))\n",
        "\n",
        "    images = torch.stack(images, dim=0).to(device)\n",
        "    images = torch.concat((torch.zeros(batch_size-images.shape[0],images.shape[1],images.shape[2]),images),dim=0)\n",
        "\n",
        "    pt = PatchEmbeddings(params)\n",
        "    patched_images = pt.patchify(images)\n",
        "    eos_reached = torch.tensor([False] * batch_size, device=device)\n",
        "    prompt_tokens_mask = tokens != pad_id\n",
        "    token_storage_multimodal = torch.zeros(patched_images.size(0), patched_images.size(1)).to(device)\n",
        "\n",
        "    for cur_pos in tqdm(range(1, total_len), desc='Generating tokens'):\n",
        "        with torch.no_grad():\n",
        "            logits = model.forward(tokens[:, cur_pos-1:cur_pos], None, cur_pos, False)\n",
        "        if temp > 0:\n",
        "            probs = torch.softmax(logits[:, -1] / temp, dim=-1)\n",
        "            next_token = _sample_top_p(probs, top_p)\n",
        "        else:\n",
        "            next_token = torch.argmax(logits[:, -1], dim=-1)\n",
        "        next_token = next_token.reshape(-1)\n",
        "        next_token = torch.where(prompt_tokens_mask[:, cur_pos], tokens[:, cur_pos], next_token)\n",
        "        tokens[:, cur_pos] = next_token\n",
        "        eos_reached |= (~prompt_tokens_mask[:, cur_pos]) & (next_token == tokenizer.eos_token_id)\n",
        "        if all(eos_reached):\n",
        "            break\n",
        "\n",
        "    for cur_pos in tqdm(range(1, patched_images.size(1)), desc='Generating tokens'):\n",
        "        with torch.no_grad():\n",
        "            logits = model.forward(None, patched_images[:, cur_pos-1:cur_pos], cur_pos, True)\n",
        "        if temp > 0:\n",
        "            probs = torch.softmax(logits[:, -1] / temp, dim=-1)\n",
        "            next_token = _sample_top_p(probs, top_p)\n",
        "        else:\n",
        "            next_token = torch.argmax(logits[:, -1], dim=-1)\n",
        "\n",
        "        next_token = next_token.reshape(-1)\n",
        "        token_storage_multimodal[:, cur_pos] = next_token\n",
        "        eos_reached |=  (next_token == tokenizer.eos_token_id)\n",
        "        if all(eos_reached):\n",
        "            break\n",
        "\n",
        "    out_tokens = []\n",
        "    out_text = []\n",
        "    for prompt_index, current_prompt_tokens in enumerate(token_storage_multimodal.tolist()):\n",
        "        if tokenizer.eos_token_id in current_prompt_tokens:\n",
        "            eos_idx = current_prompt_tokens.index(tokenizer.eos_token_id)\n",
        "            current_prompt_tokens = current_prompt_tokens[:eos_idx]\n",
        "        out_tokens.append(current_prompt_tokens)\n",
        "        out_text.append(tokenizer.decode(torch.tensor(current_prompt_tokens, dtype=torch.int32)))\n",
        "\n",
        "    for prompt_index, current_prompt_tokens in enumerate(tokens.tolist()):\n",
        "        if tokenizer.eos_token_id in current_prompt_tokens:\n",
        "            eos_idx = current_prompt_tokens.index(tokenizer.eos_token_id)\n",
        "            current_prompt_tokens = current_prompt_tokens[:eos_idx]\n",
        "        out_tokens.append(current_prompt_tokens)\n",
        "        out_text.append(tokenizer.decode(current_prompt_tokens))\n",
        "\n",
        "\n",
        "    return (out_tokens, out_text)\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "#tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "\n",
        "#hf_checkpoint = \"Intel/llava-llama-4-8b\"\n",
        "#tokenizer = AutoTokenizer.from_pretrained(hf_checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
        "print(tokenizer.vocab_size)\n",
        "#processor = AutoProcessor.from_pretrained(hf_checkpoint)\n",
        "params = Parameters(device = device, use_cache = False, num_heads = 16, thresh = None, emb_dim = 256, max_seq_len = 256\n",
        "                    ,ffn_hidden_dim = 512, batch_size = 8, div_batch = 8,\n",
        "                    tokenizer = tokenizer, vocab_size = tokenizer.vocab_size+1,\n",
        "                    max_im_width = 240, max_im_height = 240, num_layers = 1, patch_size = 16, dataset_name = \"ct_scan_data\",\n",
        "                    token_thresh = 0.3, imp_layer_hidden = 512)\n",
        "\n",
        "dataloader = DataLoader(\n",
        "    Medical_Data(params),\n",
        "    batch_size=params.batch_size,\n",
        "    collate_fn=Medical_Data(params).collate_fn\n",
        ")\n",
        "\n",
        "tk = TokenFusion(params)\n",
        "\n",
        "#change these queries to get your own desired results\n",
        "#print(infer(tokenizer,[\"text: Hello? How are you?\",\"text: Medical Imaging right here\",\n",
        "                        #\"image: LPMC/PMC908/PMC9088011_fimmu-13-881352-g002_undivided_1_1.jpg text: What does the image describe?\"], params, tk))"
      ],
      "metadata": {
        "id": "vzuNvULw7LP-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}